"""This module contains the Assistant class that interacts with the LLM
to generate comments and hypotheses for the experiment."""

import json
import os
import re
import numpy as np
import pandas as pd
import tiktoken
from copy import deepcopy
from enum import Enum
from typing import Dict, List, Tuple
from openai import OpenAI
from bora.experiment import Experiment, Type
from .util import hashable

MODEL_INFOS = {
    "gpt-4o": {
        "prompt_tokens_price_per_1k": 0.0025,  # $ per 1,000 tokens
        "completion_tokens_price_per_1k": 0.01,  # $ per 1,000 tokens
        "max_tokens": 128_000,
    },
    "gpt-4o-mini": {
        "prompt_tokens_price_per_1k": 0.00015,
        "completion_tokens_price_per_1k": 0.0006,
        "max_tokens": 128_000,
    },
}


class CommentType(Enum):
    PREOPTIMIZATION = 0
    POINTS = 1
    CONCLUSION = 3


class BaseComment:

    def __init__(self, response: str, iteration: int) -> None:
        self._iteration = iteration
        self._comment = response
        self.type = None

    def __str__(self):
        return self._comment

    @property
    def comment(self):
        return self._comment

    @property
    def iteration(self):
        return self._iteration

    @property
    def is_valid(self):
        validity = self._comment is not None
        return validity

    def _fix_response_format(
        self,
        text_str: str,
    ) -> Tuple[bool, Dict[str, object]]:
        """
        Fix the JSON format of the response. If the JSON format is not correct,
        the LLM will be asked to fix it up to 3 times.

        Parameters:
            text_str (str): The response from the LLM.

        Returns:
        Tuple[bool, Dict[str, object]]: Whether the JSON format was fixed and
        the JSON.
        """
        text_json = None
        fixed = False
        attempt_count = 0
        hypothesis_key = "hypotheses"

        while not fixed and attempt_count < 3:
            try:
                try:
                    text_str = self._retrieve_json_response(text_str)
                except Exception:
                    pass

                text_json = json.loads(text_str)
                if "comment" in text_json and hypothesis_key in text_json:
                    fixed = True
                    return fixed, text_json
                else:
                    return False, text_json
            except json.JSONDecodeError as e:
                messages = [
                    {
                        "role": "user",
                        "content": f"I tried to convert the following JSON string into JSON:\n {text_str}\n\n But it failed with the following error:\n{e.args[0]}\n\nPlease ONLY fix formatting errors such as missing curly or square braces, commas, etc., without changing any of the actual content, values, keys, or structure of the JSON. Do not add any new fields, remove existing fields, or modify any values in the JSON string, especially for bounds and priors!! Respond with just the complete corrected JSON string without any additional comments.",
                    }
                ]

                text_str = self._chat_completion(messages)

            # Increase the attempt count
            attempt_count += 1

        return False, text_json

    def _retrieve_json_response(self, response: str) -> str:
        """
        Extract the JSON list part using a regular expression.

        Parameters
        ----------
        response: str
            The hypotheses generated by the LLM.

        Returns
        -------
        str: The JSON list part of the hypotheses in string format.
        """
        response_json_str = re.search(r"\{.*\}", response, re.DOTALL).group()

        # Clean the JSON string
        response_json_str = self._clean_json_string(response_json_str)

        return response_json_str

    def _clean_json_string(self, json_str: str) -> str:
        """
        Clean the JSON string. Remove newlines, escaped characters,
        and comments.

        Parameters
        ----------
        json_str : str
            The JSON string to clean.

        Returns
        -------
        str: The cleaned JSON string.
        """
        # Remove newlines
        cleaned_string = json_str.replace("\n", "")

        cleaned_string = cleaned_string.replace(r"\'", "")

        # Remove escaped characters
        cleaned_string = re.sub(r"\\", "", cleaned_string)

        # Move Remove comments to clean function
        cleaned_string = re.sub(r"//.*", "", cleaned_string)

        return cleaned_string


class Comment(BaseComment):

    def __init__(
        self,
        response: str,
        iteration: int,
        experiment: Experiment,
        llm_model: str,
        api_key: str,
        total_tokens,
        random_seed: int = 0,
        temperature: int = 0,
    ) -> None:
        super().__init__(response, iteration)
        self._iteration = iteration
        self._experiment = experiment
        self._pbounds = self._experiment.pbounds
        self._keys = self._experiment.keys
        self._random_seed = random_seed
        self.type = CommentType.POINTS

        # Initialize the OpenAI API client
        self._model = llm_model
        self._temperature = temperature
        self._client = OpenAI(api_key=api_key)
        self._total_tokens = total_tokens

        self._comment = None
        self._hypotheses = []
        if response is not None:
            self._process_response(response)

    def __str__(self):
        _str = json.dumps(
            {
                "comment": self._comment,
                "hypotheses": self._hypotheses,
            },
            indent=4,
        )
        return _str

    @property
    def hypotheses(self):
        return self._hypotheses

    @hypotheses.setter
    def hypotheses(self, value):
        self._hypotheses = value

    @property
    def is_valid(self):
        return (
            self._comment is not None
            and len(self._hypotheses) > 0
            and all(len(h["points"]) > 0 for h in self._hypotheses)
        )

    def _process_response(self, response: str) -> Tuple[str, List[Dict[str, object]]]:
        """
        Process the response to extract the comment; domain centric insights
        and hypotheses.

        Parameters
        ----------
        str : The response from the LLM.
        """
        # Ensure the JSON format is correct
        fixed, response_json = self._fix_response_format(response)

        if not fixed:
            return None, []

        comment = response_json["comment"]
        hypotheses = []
        for h in response_json["hypotheses"]:
            # Check that the hypothesis structure is correct
            clean, hypothesis = self._clean_hypothesis_keys(h)
            if not clean:
                continue

            hypotheses.append(hypothesis)
        self._comment = comment.strip()
        self._hypotheses = hypotheses

    def _clean_hypothesis_keys(self, hypothesis: Dict[str, object]) -> str:
        """
        Clean the keys of the hypothesis. If the hypothesis is not in the
        correct format, it will be removed.

        Parameters
        ----------
        hypothesis : Dict[str, object]
            The hypothesis to clean.

        Returns
        --------
        str: The cleaned hypothesis.
        """
        cleaned_hypothesis = {}

        # Check name
        if "name" in hypothesis:
            cleaned_hypothesis["name"] = hypothesis["name"]
        else:
            return False, cleaned_hypothesis

        # Check rationale
        if "rationale" in hypothesis:
            cleaned_hypothesis["rationale"] = hypothesis["rationale"]
        else:
            return False, cleaned_hypothesis

        # Check score
        if "confidence" in hypothesis:
            cleaned_hypothesis["confidence"] = hypothesis["confidence"]
        else:
            return False, cleaned_hypothesis

        # Check points
        if "points" in hypothesis:
            cleaned_hypothesis["points"] = []
            for p in hypothesis["points"]:
                if not isinstance(p, dict):
                    return False, cleaned_hypothesis
                if len(p.keys()) != len(self._keys):
                    return False, cleaned_hypothesis
                for k, v in p.items():
                    parameter = self._experiment.get_parameter(k)
                    if k not in self._keys:
                        return False, cleaned_hypothesis
                    if not parameter.is_valid_value(v):
                        return False, cleaned_hypothesis
                cleaned_hypothesis["points"].append(p)
        else:
            return False, cleaned_hypothesis

        return True, cleaned_hypothesis

    def _chat_completion(self, messages: List[Dict[str, str]]) -> str:
        """
        Generate a completion based on the messages.

        Parameters
        ----------
        messages : List[Dict[str, str]]
            The messages in the chat.

        Returns
        -------
        str: The completion generated by the LLM.
        """
        completion = self._client.chat.completions.create(
            model=self._model,
            messages=messages,
            seed=self._random_seed,
            # temperature=self._temperature,
        )
        content = completion.choices[0].message.content
        self._total_tokens.append(
            (completion.usage.prompt_tokens, completion.usage.completion_tokens)
        )
        return content


class Assistant:
    """This class interacts with the LLM to generate comments
    and hypotheses for the experiment."""

    def __init__(
        self,
        api_key: str,
        experiment: Experiment,
        random_point: np.ndarray,
        log_path: str,
        llm_model: str = "gpt-4o-mini",
        allow_duplicate_points: bool = False,
        random_seed: int = 0,
        temperature: int = 0,
        save_prompts: bool = False,
    ):
        # Experiment
        self._random_seed = random_seed
        self._experiment = experiment
        self._random_point = random_point
        self._constraint = (
            experiment.constraint.constraint if experiment.constraint else None
        )
        self._allow_duplicate_points = allow_duplicate_points
        self._pbounds = {p.name: p.get_bounds() for p in self._experiment.parameters}
        self._keys = [p.name for p in self._experiment.parameters]
        self._cache = {}
        # Initialize the OpenAI API client
        # TODO hide the API key to pass to the Comment class
        self._api_key = api_key
        self._client = OpenAI(api_key=self._api_key)
        self._model = llm_model
        self._model_info = self._get_model_info()
        self._temperature = temperature

        # LLM role based prompting
        self._chat_history = [
            {
                "role": "system",
                "content": (
                    "You are BORA (Bayesian Optimization Research Assistant), "
                    "a scientific assistant specialized in Bayesian "
                    "Optimization and a live commentator. Your role is to comment "
                    "on the experimental progress and generate hypotheses."
                ),
            }
        ]
        self._total_tokens = []

        # Logs
        self._log_path = self._init_log_path(log_path)
        self._comments = []
        self._experiment_overview = None
        self._summary_path = self._log_path.replace("comments", "summary")
        self._save_prompts = save_prompts

        # Set the float format to match the experiment default precision
        pd.options.display.float_format = (
            f"{{:.{self._experiment.default_precision}f}}".format
        )

    @property
    def experiment_overview(self):
        return self._experiment_overview

    @property
    def last_comment(self):
        if len(self._comments) == 0:
            return None
        return self._comments[-1]

    @property
    def conclusion(self):
        return self._conclusion

    @property
    def optimization_comments(self):
        comments = [c for c in self._comments if c.iteration != -1]
        return comments

    def _init_log_path(self, log_path: str) -> str:
        # Get log_path extension
        _, file_extension = os.path.splitext(log_path)
        if file_extension != ".md":
            log_path += ".md"

        # Remove previous logs with the same identifier
        identifier = (
            f"{self._experiment.name}_d{self._experiment.dim}_s{self._random_seed}"
        )
        for file in os.listdir(os.path.dirname(log_path)):
            if identifier in file:
                os.remove(os.path.join(os.path.dirname(log_path), file))
        os.makedirs(os.path.dirname(log_path), exist_ok=True)

        return log_path

    def _save_prompt(self, prompt: str, path: str):
        with open(path, "w", encoding="utf-8") as f:
            f.write(prompt)

    def _generate_experiment_overview_prompt(self) -> str:
        """Generate the experiment overview prompt.

        Returns:
            str: The experiment overview prompt.
        """
        prompt_path = os.path.join(
            "bora",
            "prompts",
            "experiment_overview_prompt.txt",
        )
        prompt = open(prompt_path, "r").read()
        prompt = self._add_target_to_prompt(prompt)
        prompt = self._add_description_to_prompt(prompt)
        prompt = self._add_parameters_to_prompt(prompt)
        prompt = self._add_constraint_to_prompt(prompt)

        # Save prompt
        if self._save_prompts:
            path = self._log_path.replace(
                "comments", "experiment_overview_prompt"
            ).replace("md", "txt")
            with open(path, "w", encoding="utf-8") as f:
                f.write(prompt)
        return prompt

    def _generate_starter_prompt(self, n_hypotheses: int) -> str:
        """
        Generate the starter prompt for the experiment.

        Parameters
        ----------
        n_hypotheses : int
            The number of hypotheses to generate.

        Returns
        --------
        str: The starter prompt.
        """
        prompt_path = os.path.join(
            "bora",
            "prompts",
            "starter_prompt.txt",
        )
        prompt = open(prompt_path, "r").read()
        prompt = self._add_target_to_prompt(prompt)
        prompt = self._add_description_to_prompt(prompt)
        prompt = self._add_parameters_to_prompt(prompt)
        prompt = self._add_constraint_to_prompt(prompt)
        prompt = self._add_domain_to_prompt(prompt)

        constraint_violation = ""
        if self._constraint:
            constraint_violation = (
                f" for which {self._experiment.constraint.description}"
            )
            if self._experiment.type == Type.discrete:
                constraint_violation += ", and the discretization step sizes of the parameters are respected"
        elif self._experiment.type == Type.discrete:
            constraint_violation = (
                " which satisfies the discretization step sizes of the parameters"
            )
        prompt = prompt.replace("[constraint_violation]", constraint_violation)

        parameter_1 = self._experiment.parameters[0].name
        parameter_2 = self._experiment.parameters[1].name
        value_1 = self._random_point[0]
        value_2 = self._random_point[1]
        prompt = prompt.replace("[parameter_1]", parameter_1)
        prompt = prompt.replace("[parameter_2]", parameter_2)
        prompt = prompt.replace(
            "[value_1]", f"{value_1:.{self._experiment.default_precision}f}"
        )
        prompt = prompt.replace(
            "[value_2]", f"{value_2:.{self._experiment.default_precision}f}"
        )
        prompt = prompt.replace("[n_hypotheses]", str(n_hypotheses))

        # Save prompt
        if self._save_prompts:
            path = self._log_path.replace("comments", "starter_prompt").replace(
                "md", "txt"
            )
            self._save_prompt(prompt, path)
        return prompt

    def _generate_comment_prompt(self, data: pd.DataFrame) -> str:
        """
        Generate a comment prompt based on the data.

        Parameters
        ----------
        data (pd.DataFrame): The data from the experiment.

        Returns
        -------
        str: The comment prompt.
        """
        prompt_path = os.path.join(
            "bora",
            "prompts",
            "comment_prompt.txt",
        )
        prompt = open(prompt_path, "r").read()
        prompt = prompt.replace("[iteration]", str(len(data)))

        previous_comments = ""
        new_data_count = len(data)
        if len(self.optimization_comments) > 0:
            previous_comments = (
                "You generated the following comments in the previous "
                "iterations and your suggestion points were subsequently "
                "evaluated and appended to the dataset:\n"
            )
            for comment in self.optimization_comments:
                previous_comments += (
                    f"## Iteration {comment.iteration}:\n" + f"{comment}\n"
                )
            new_data_count = (
                new_data_count - self.optimization_comments[-1].iteration + 1
            )
        prompt = prompt.replace("[previous_comments]", previous_comments)
        prompt = prompt.replace("[new_data_count]", str(new_data_count))
        prompt = self._add_target_to_prompt(prompt)
        prompt = self._add_constraint_violation_to_prompt(prompt)

        # Data
        data_count = len(data)
        prompt = prompt.replace("[data_count]", str(data_count))
        dataset = data.to_string(index=False)
        # ---- We need to ensure that the dataset fits in the context window
        # Otherwise we truncate it
        res = self._would_context_be_full(prompt + dataset)
        if res is True:
            # Reduce the dataset to fit in the context window
            data = self._summarize_dataset(data)
            dataset = "The corresponding correlation matrix is:\n" + data.to_string(
                index=False
            )
        prompt = prompt.replace("[dataset]", dataset)

        # Save prompt
        if self._save_prompts:
            path = self._log_path.replace(
                "comments", f"comment_prompt_at_{data_count}"
            ).replace("md", "txt")
            self._save_prompt(prompt, path)
        return prompt

    def _generate_comment_selection_prompt(
        self,
        data: pd.DataFrame,
        suggestions: pd.DataFrame,
    ) -> str:
        """
        Generate a comment prompt based on the data.

        Parameters
        ----------
        data (pd.DataFrame): The data from the experiment.

        Returns
        -------
        str: The comment prompt.
        """
        prompt_path = os.path.join(
            "bora",
            "prompts",
            "comment_selection_prompt.txt",
        )
        prompt = open(prompt_path, "r").read()
        prompt = prompt.replace("[iteration]", str(len(data)))

        previous_comments = ""
        new_data_count = len(data)
        last_comment_iteration = 0
        if len(self.optimization_comments) > 0:
            previous_comments = (
                "You generated the following comments in the previous "
                "iterations and your suggestion points were subsequently "
                "evaluated and appended to the dataset:\n"
            )
            for comment in self.optimization_comments:
                previous_comments += (
                    f"## Iteration {comment.iteration}:\n" + f"{comment}\n"
                )
            new_data_count = (
                new_data_count - self.optimization_comments[-1].iteration + 1
            )
            last_comment_iteration = self.optimization_comments[-1].iteration
        prompt = prompt.replace("[previous_comments]", previous_comments)
        prompt = prompt.replace("[new_data_count]", str(new_data_count))
        prompt = self._add_target_to_prompt(prompt)
        prompt = self._add_constraint_violation_to_prompt(prompt)

        # Data
        data_count = len(data)
        prompt = prompt.replace("[data_count]", str(data_count))
        dataset = data.to_string(index=False)

        # ---- We need to ensure that the dataset fits in the context window
        # Otherwise we truncate it
        res = self._would_context_be_full(prompt + dataset)
        if res is True:
            # Reduce the dataset to fit in the context window
            data = self._summarize_dataset(data)
            dataset = "The corresponding correlation matrix is:\n" + data.to_string(
                index=False
            )
        prompt = prompt.replace("[dataset]", dataset)
        prompt = prompt.replace("[last_comment_iteration]", str(last_comment_iteration))
        prompt = prompt.replace("[suggestions]", suggestions.to_string(index=False))

        # Save prompt
        if self._save_prompts:
            path = self._log_path.replace(
                "comments", f"comment_selection_prompt_at_{data_count}"
            ).replace("md", "txt")
            self._save_prompt(prompt, path)
        return prompt

    def _generate_conclusion_prompt(self, data: pd.DataFrame) -> str:
        """
        Generate a conclusion prompt based on the data.

        Parameters
        ----------
        data (pd.DataFrame): The data from the experiment.

        Returns
        -------
        str: The conclusion prompt.
        """
        prompt_path = os.path.join(
            "bora",
            "prompts",
            "conclusion_prompt.txt",
        )
        prompt = open(prompt_path, "r").read()

        previous_comments = ""
        comments = [c for c in self._comments if c.iteration != -1]
        if len(comments) > 0:
            previous_comments = (
                "You generated the following comments in the previous"
                + " iterations:\n"
            )
            for comment in comments:
                # Turn comment dict into string
                str_comment = str(comment)
                previous_comments += (
                    f"## Iteration {comment.iteration}:\n {str_comment}\n"
                )
        prompt = prompt.replace("[previous_comments]", previous_comments)

        # Data
        data_count = len(data)
        prompt = prompt.replace("[data_count]", str(data_count))
        dataset = data.to_string(index=False)

        # ---- We need to ensure that the dataset fits in the context window
        # Otherwise we truncate it
        res = self._would_context_be_full(prompt + dataset)
        if res is True:
            # Reduce the dataset to fit in the context window
            data = self._summarize_dataset(data)
            dataset = "The corresponding correlation matrix is:\n" + data.to_string(
                index=False
            )
        prompt = prompt.replace("[dataset]", dataset)

        # Save prompt
        if self._save_prompts:
            path = self._log_path.replace("comments", "conclusion_prompt").replace(
                "md", "txt"
            )
            self._save_prompt(prompt, path)
        return prompt

    def _generate_summary_prompt(self, maximum: float) -> str:
        """Generate the summary prompt.

        Returns
        -------
        str: The summary prompt.
        """
        prompt_path = os.path.join(
            "bora",
            "prompts",
            "summary_prompt.txt",
        )
        prompt = open(prompt_path, "r").read()
        prompt = self._add_domain_to_prompt(prompt)
        prompt = self._add_target_to_prompt(prompt)
        prompt = prompt.replace("[maximum]", str(maximum))

        # Save prompt
        if self._save_prompts:
            path = self._log_path.replace("comments", "summary_prompt").replace(
                "md", "txt"
            )
            self._save_prompt(prompt, path)
        return prompt

    def _add_target_to_prompt(self, prompt: str) -> str:
        """Add the target to the prompt.

        Parameters
        ----------
        prompt : str
            The prompt to add the target to.

        Returns:
        --------
        str: The prompt with the target added.
        """
        target = self._experiment.target.name
        prompt = prompt.replace("[target]", target)
        return prompt

    def _add_description_to_prompt(self, prompt: str) -> str:
        """Add the description to the prompt.

        Parameters
        ----------
        prompt : str
            The prompt to add the description to.

        Returns
        -------
        str: The prompt with the description added.
        """
        prompt = prompt.replace("[description]", self._experiment.description)
        return prompt

    def _add_parameters_to_prompt(self, prompt: str) -> str:
        """Add the parameters to the prompt.

        Parameters
        ----------
        prompt : str
            The prompt to add the parameters to.

        Returns
        -------
        str: The prompt with the parameters added.
        """
        parameters_and_bounds = ""

        if self._experiment.type == Type.categorical:
            for parameter in self._experiment.parameters:
                parameters_and_bounds += f"- {parameter.name}: "
                parameters_and_bounds += f"{parameter.description} "
                parameters_and_bounds += f"This parameter has the following categories: {parameter.categories}.\n"
        else:
            for parameter in self._experiment.parameters:
                parameters_and_bounds += f"- {parameter.name}: "
                parameters_and_bounds += f"{parameter.description}. "
                parameters_and_bounds += (
                    f"This parameter bounds are {parameter.get_bounds()}."
                )
                if parameter.type == Type.discrete:
                    parameters_and_bounds += (
                        f" Its discretization step size is {parameter.step}."
                    )
                parameters_and_bounds += "\n"
        prompt = prompt.replace(
            "[parameters_and_bounds]",
            parameters_and_bounds,
        )
        return prompt

    def _add_constraint_to_prompt(self, prompt: str) -> str:
        """Add the constraint to the prompt.

        Parameters
        ----------
        prompt : str
            The prompt to add the constraint to.

        Returns
        -------
        str: The prompt with the constraint added.
        """
        constraint_desc = ""
        if self._experiment.constraint:
            constraint_desc = self._experiment.constraint.description
            constraint_desc = (
                "The constraint of the experiment is that " + constraint_desc
            )
        prompt = prompt.replace("[constraint]", constraint_desc)
        return prompt

    def _add_constraint_violation_to_prompt(self, prompt: str) -> str:
        """
        Add the constraint violation to the prompt.

        Parameters
        prompt : str
            The prompt to add the constraint violation to.

        Returns
        -------
        str: The prompt with the constraint violation added.
        """
        constraint_violation = ""
        if self._constraint:
            constraint_violation = " which satisfies the experiment " + "constraint,"
        prompt = prompt.replace("[constraint_violation]", constraint_violation)
        return prompt

    def _add_domain_to_prompt(self, prompt: str) -> str:
        """Add the domain to the prompt.

        Parameters
        ----------
        prompt : str
            The prompt to add the domain to.

        Returns
        -------
        str: The prompt with the domain added.
        """
        prompt = prompt.replace("[domain]", self._experiment.domain)
        return prompt

    def _add_user_message(self, message: str):
        """
        Add a user message to the chat history.

        Parameters
        ----------
        message : str
        _description_
        """
        self._chat_history.append({"role": "user", "content": message})

    def _add_assistant_message(self, message: str):
        """
        Add a system message to the chat history.

        Parameters:
            message (str): The message to add.
        """
        self._chat_history.append({"role": "assistant", "content": message})

    def _summarize_dataset(self, data: pd.DataFrame) -> str:
        """
        Summarize the dataset.

        Parameters
        ----------
        data (pd.DataFrame): The data to summarize.

        Returns
        -------
        str: The summary of the dataset.
        """
        data.set_index("iteration", inplace=True)
        correlation_matrix = data.corr()
        return correlation_matrix

    def _get_model_info(self) -> int:
        if self._model in MODEL_INFOS:
            info = MODEL_INFOS[self._model]
            encoding = tiktoken.encoding_for_model(self._model)
            info["encoding"] = encoding
            return info
        else:
            raise ValueError(f"Model {self._model} not found in MODEL_INFOS")

    def _get_token_count(self, message: str) -> int:
        """
        Get the number of tokens in the messages.

        Parameters
        ----------
        message : str
            The messages in the chat.

        Returns
        -------
        int: The number of tokens in the messages.
        """
        num_tokens = len(self._model_info["encoding"].encode(message))
        return num_tokens

    def _would_context_be_full(self, input):
        """
        Check if the context would be full after adding the input.

        Parameters
        ----------
        input : str
            The input to add.

        Returns
        -------
        bool: Whether the context would be full.
        """
        num_tokens = self._get_token_count(input)
        res = num_tokens > 0.9 * self._model_info["max_tokens"]
        return res

    def _get_all_tokens(self):
        """
        Get the total number of tokens used.

        Returns
        -------
        int: The total number of tokens used.
        """
        input_tokens, output_tokens = zip(*self._total_tokens)
        input_tokens = sum(input_tokens)
        output_tokens = sum(output_tokens)
        return input_tokens, output_tokens

    def _get_price(self):
        """
        Get the price of the tokens used.

        Returns
        -------
        float: The price of the tokens used.
        """
        input_tokens, output_tokens = self._get_all_tokens()
        total_price = (
            input_tokens * self._model_info["prompt_tokens_price_per_1k"]
            + output_tokens * self._model_info["completion_tokens_price_per_1k"]
        )
        total_price /= 1000
        return total_price

    def _chat_completion(
        self,
        messages: List[Dict[str, str]],
        n: int = 1,
    ) -> str:
        """
        Generate a completion based on the messages.

        Parameters:
            messages (List[Dict[str, str]]): The messages in the chat.

        Returns:
            str: The completion generated by the LLM.
        """
        # TODO test internet or request time out error
        try:
            completion = self._client.chat.completions.create(
                model=self._model,
                messages=messages,
                seed=self._random_seed,
                n=n,
                # temperature=self._temperature,
            )
            self._total_tokens.append(
                (completion.usage.prompt_tokens, completion.usage.completion_tokens)
            )
        except Exception as e:
            print(f"Error while generating completion: {e.args}")
            return None
        if n > 1:
            contents = [c.message.content for c in completion.choices]
            content = f"Here are {n} possible comments I generated:\n-" + "\n-".join(
                contents
            )
            messages_ = deepcopy(messages) + [{"role": "system", "content": content}]
            based_on_data_too = ""
            if self.last_comment:
                based_on_data_too = "and correct them based on the dataset. "
            messages_.append(
                {
                    "role": "user",
                    "content": (
                        "Please fact check the claims, the maximum value, etc. "
                        + based_on_data_too
                        + "Reflect, critique and merge the comments into one promising "
                        "comment to maximize the target.\nImportant: "
                        "Only provide your response in the exact JSON response "
                        "format described at the beginning of our chat, without "
                        "any additional syntax or libraries.\n```json"
                    ),
                }
            )
            try:
                completion = self._client.chat.completions.create(
                    model=self._model,
                    messages=messages_,
                    seed=self._random_seed,
                    # temperature=self._temperature,
                )
                self._total_tokens.append(
                    (completion.usage.prompt_tokens, completion.usage.completion_tokens)
                )
            except Exception as e:
                print(f"Error while generating completion: {e.args}")
                return None

        content = completion.choices[0].message.content.strip()
        return content

    def _check_for_duplicates(
        self,
        comment: Comment,
        cache: Dict,
    ) -> Comment:
        """
        Check for duplicate points in the hypotheses. If a hypothesis has
        duplicates of previous points, the LLM will be asked to provide new
        points.

        Parameters
        ----------
        hypotheses (List[Dict[str, object]]): The hypotheses to check.
        cache (Dict): The cache of previous points.

        Returns
        -------
        Comment: The comment with the hypotheses without duplicates.
        """
        n_tries = 0
        while n_tries < 5:
            message = ""
            h_points_hashes = []
            for h in comment.hypotheses:
                h_msg = ""
                for p in h["points"]:
                    _hash = hashable(list(p.values()))
                    if _hash in cache or _hash in h_points_hashes:
                        h_msg += f"\n- {p}"
                    h_points_hashes.append(_hash)
                if len(h_msg) > 0:
                    message += f"\nHypothesis {h['name']} has the following duplicate points: {h_msg}"
            if len(message) > 0:
                # Add the comment to the chat history
                self._add_assistant_message(str(comment))

                # Add the deduplication message to the chat history
                message += (
                    "\nPlease provide new points that are not duplicates"
                    " of points from previous iterations. In your updated"
                    " comment do not mention the resolution of the "
                    "duplicates as that is an internal work the users do "
                    "not need to know about. \nImportant: Only provide "
                    "your response in the exact JSON response format described"
                    " at the beginning of our chat, without any additional "
                    "syntax or libraries. The points must have values for all the parameters of the experiment.\n```json"
                )
                self._add_user_message(message)
                response = self._chat_completion(self._chat_history)
                comment = Comment(
                    response,
                    comment.iteration,
                    self._experiment,
                    self._model,
                    self._api_key,
                    self._total_tokens,
                    self._random_seed,
                )
                self._chat_history.pop()  # Remove the deduplication message
                self._chat_history.pop()  # Remove the comment message
                n_tries += 1
            else:
                return comment

        # Remove the duplicates
        hypotheses = comment.hypotheses
        for i in range(len(hypotheses) - 1, -1, -1):
            h = hypotheses[i]
            h_points_hashes = []
            for j in range(len(h["points"]) - 1, -1, -1):
                p = h["points"][j]
                _hash = hashable(list(p.values()))
                if _hash in cache or _hash in h_points_hashes:
                    h["points"].pop(j)
                h_points_hashes.append(_hash)
            if len(h["points"]) == 0:
                hypotheses.pop(i)
        comment.hypotheses = hypotheses
        return comment

    def _constraint_violation(self, comment: Comment) -> Comment:
        """
        Check for constraint violations in the comment hypotheses. If a point
        violates the constraint, it will be removed. If a hypothesis has no
        points left, it will be removed.

        Parameters
        ----------
        comment : Comment
            The comment to check.

        Returns
        --------
        Comment : The comment with the hypotheses without constraint
            violations.
        """
        message = ""
        for h in comment.hypotheses:
            points = h["points"]
            h_name = h["name"]
            invalid_points = self._get_invalid_points(points)
            if len(invalid_points) > 0:
                h_msg = f"The following points in hypothesis '{h_name}' violate the requirements of the experiment:"
                for point, value, reason in invalid_points:
                    if reason == "constraint":
                        h_msg += (
                            f"\n- {point} violates the constraint with value {value}"
                        )
                    else:
                        h_msg += f"\n- {point} is {reason}"
                message += f"{h_msg}.\n"

        if len(message) > 0:
            message += """\nPlease provide new points that satisfy the constraint of the experiment. In your updated comment do not mention the resolution of the constraint violation s as that is an internal work the users do not need to know about.\n Important: Only provide your response in the exact JSON response format described at the beginning of our chat, without any additional syntax or libraries. The points must have values for all the parameters of the experiment.\n```json"""
            self._add_user_message(message)
            response = self._chat_completion(self._chat_history)
            comment = Comment(
                response,
                comment.iteration,
                self._experiment,
                self._model,
                self._api_key,
                self._total_tokens,
                self._random_seed,
            )
            self._chat_history.pop()  # Remove last message

            # Delete the points that violate the constraint
            hypotheses = comment.hypotheses
            for i in range(len(hypotheses) - 1, -1, -1):
                h = hypotheses[i]
                h = self._delete_invalid_points(h)
                if len(h["points"]) == 0:
                    hypotheses.pop(i)
            comment.hypotheses = hypotheses

        return comment

    def _get_invalid_points(
        self, points: List[List[float]]
    ) -> List[Tuple[List[float], float]]:
        """
        Get the points that violate the constraint or the bounds.

        Parameters
        ----------
        points : List[List[float]]
            The points to check.

        Returns
        --------
        List[Tuple[List[float], float]]: The points that violate the
        constraint and their constraint values.
        """
        invalid_points = []
        for point in points:
            # Check the constraint
            if self._constraint:
                value = self._constraint.eval(**point)
                allowed = self._constraint.allowed(value)[0]
                if not allowed:
                    invalid_points.append((point, value, "constraint"))

            # Check the bounds
            for param in self._experiment.parameters:
                bounds = param.get_bounds()
                value = point[param.name]
                if value < bounds[0] or value > bounds[1]:
                    reason = f"outside of the bounds of {param.name} which are {bounds}"
                    invalid_points.append((point, value, reason))
        return invalid_points

    def _delete_invalid_points(
        self, hypothesis: Dict[str, object]
    ) -> Dict[str, object]:
        """
        Delete the points that violate the constraint from the hypothesis.

        Parameters
        ----------
        hypothesis : Dict[str, object]
            The hypothesis to check.

        Returns
        -------
        Dict[str, object]: The hypothesis without the invalid points.
        """
        for i in range(len(hypothesis["points"]) - 1, -1, -1):
            p = hypothesis["points"][i]

            # Check the constraint
            if self._constraint:
                value = self._constraint.eval(**p)
                allowed = self._constraint.allowed(value)[0]
                if not allowed:
                    hypothesis["points"].pop(i)

            # Check the bounds
            for param in self._experiment.parameters:
                bounds = param.get_bounds()
                value = p[param.name]
                if value < bounds[0] or value > bounds[1]:
                    hypothesis["points"].pop(i)
                    break

        return hypothesis

    def _update_comment_log_file(self):
        """
        Save the last comment to a markdown file.
        """
        markdown_content = ""
        last_comment = self.last_comment

        # Pre-optimization
        if last_comment.type == CommentType.PREOPTIMIZATION:
            markdown_content += (
                f"# {self._experiment.name}\n\n"
                "## Experiment Overview\n\n"
                f"{self._experiment_overview}\n\n"
                "## Initial Thoughts\n\n"
                f"{last_comment.comment}"
            )
            hypotheses_markdown = self._point_hypothesis_markdown(
                last_comment.hypotheses
            )
            markdown_content += "\n### Initial Hypotheses\n" f"{hypotheses_markdown}"
        # During optimization
        elif last_comment.type == CommentType.POINTS:
            hypotheses_markdown = self._point_hypothesis_markdown(
                last_comment.hypotheses
            )
            markdown_content += (
                f"\n## Iteration {last_comment.iteration}\n\n"
                f"{last_comment.comment}\n\n"
                "### Updated Hypotheses\n"
                f"{hypotheses_markdown}"
            )
        # Conclusion
        elif last_comment.type == CommentType.CONCLUSION:
            markdown_content += "\n## Final Conclusions\n\n" f"{last_comment.comment}"
        with open(self._log_path, "a", encoding="utf-8") as file:
            file.write(markdown_content)

    def _point_hypothesis_markdown(self, hypotheses: Dict[str, object]) -> str:
        """Generate a markdown string for a hypothesis."""
        markdown = ""
        for hypothesis in hypotheses:
            name = hypothesis["name"]
            rationale = hypothesis["rationale"]
            confidence = hypothesis["confidence"]
            points = hypothesis["points"]

            markdown += (
                f"\n#### {name}\n\n"
                f"*Rationale:* {rationale}\n\n"
                f"*Confidence:* {confidence}\n\n"
                "*Points:*\n"
            )
            for point in points:
                markdown += f"\n- {point}\n"
        return markdown

    def pre_optimization_comment(self, n_hypotheses: int) -> Comment:
        """
        Generate the initial comment for the experiment.

        Parameters
        ----------
        n_hypotheses : int
            The number of hypotheses to generate.

        Returns
        -------
        Comment: The initial comment.
        """
        # Experiment Overview
        experiment_overview_prompt = self._generate_experiment_overview_prompt()
        self._add_user_message(experiment_overview_prompt)
        self._experiment_overview = self._chat_completion(self._chat_history)
        self._add_assistant_message(self._experiment_overview)

        # Initial Thoughts
        starter_prompt = self._generate_starter_prompt(n_hypotheses)
        self._add_user_message(starter_prompt)

        for attempt in range(3):
            valid = False  # Flag to check if the comment is valid
            response = self._chat_completion(self._chat_history, n=3)
            if response is not None:
                comment = Comment(
                    response,
                    -1,
                    self._experiment,
                    self._model,
                    self._api_key,
                    self._total_tokens,
                    self._random_seed,
                )
                if comment is None or not comment.is_valid:
                    continue  # Try again

                comment.type = CommentType.PREOPTIMIZATION
                comment = self._constraint_violation(comment)
                valid = comment.is_valid

            # Save the comment
            if valid:
                self._add_assistant_message(str(comment))
                self._comments.append(comment)
                self._update_comment_log_file()
                return comment

        raise ValueError("Failed to generate a valid comment after 3 attempts")

    def comment_optimization(
        self, data: pd.DataFrame, cache: Dict
    ) -> Tuple[str, List[Dict[str, object]]]:
        """
        Generate a comment based on the data.

        Parameters
        ----------
        data : pd.DataFrame
            The data from the experiment.
        cache: Dict
            The cache of previous points.

        Returns
        --------
        Comment: The comment.
        """
        # Generate the comment prompt
        comment_prompt = self._generate_comment_prompt(data)
        self._add_user_message(comment_prompt)

        # Generate the comment
        for attempt in range(3):
            response = self._chat_completion(self._chat_history, n=3)
            comment = Comment(
                response,
                len(data) + 1,
                self._experiment,
                self._model,
                self._api_key,
                self._total_tokens,
                self._random_seed,
            )
            if comment is None or not comment.is_valid:
                continue  # Try again

            # Check for duplicates
            comment = self._check_for_duplicates(comment, cache)

            # Check for constraint violation
            comment = self._constraint_violation(comment)

            if comment is None or not comment.is_valid:
                continue  # Try again

            # Save the comment to the comment log
            self._comments.append(comment)
            self._update_comment_log_file()
            self._chat_history.pop()  # Remove the comment prompt
            return comment

        # Remove the comment prompt as it was not valid
        self._chat_history.pop()
        return None

    def comment_and_select_point(
        self,
        data: pd.DataFrame,
        suggestions: pd.DataFrame,
    ):
        # Generate the comment prompt
        comment_prompt = self._generate_comment_selection_prompt(data, suggestions)
        self._add_user_message(comment_prompt)

        # Generate the comment
        for attempt in range(3):
            response = self._chat_completion(self._chat_history, n=3)
            comment = Comment(
                response,
                len(data) + 1,
                self._experiment,
                self._model,
                self._api_key,
                self._total_tokens,
                self._random_seed,
            )
            if comment is None or not comment.is_valid:
                self._chat_history.pop()  # Remove the comment prompt
                continue  # Try again

            # Remove the points that are not in the suggestions
            hypotheses = comment.hypotheses
            for i in range(len(hypotheses) - 1, -1, -1):
                h = hypotheses[i]
                points = h["points"]
                for j in range(len(points) - 1, -1, -1):
                    p = np.array(list(points[j].values()))
                    records = np.around(
                        suggestions.to_numpy(), self._experiment.default_precision
                    )
                    # Check if p is in the suggestions
                    if not any(np.all(p == r) for r in records):
                        points.pop(j)
                if len(points) == 0:
                    hypotheses.pop(i)

            self._chat_history.pop()  # Remove the comment prompt
            if comment is None or not comment.is_valid:
                continue  # Try again

            # Save the comment to the comment log
            self._comments.append(comment)
            self._update_comment_log_file()
            return comment

        return None

    def conclude(self, data: pd.DataFrame):
        """Generate a conclusion based on the data.

        Parameters
        ----------
            data (pd.DataFrame): The data from the experiment.
        """
        prompt = self._generate_conclusion_prompt(data)
        self._add_user_message(prompt)
        conclusion = self._chat_completion(self._chat_history)
        comment = BaseComment(conclusion, -2)
        comment.type = CommentType.CONCLUSION
        self._add_assistant_message(conclusion)
        self._comments.append(comment)
        self._update_comment_log_file()

    def save_summary(self, maximum) -> str:
        """Generate and save the summary to the path to the summary file
        which the same as the log but starting with 'summary'
        """
        # Generate summary
        summary_prompt = self._generate_summary_prompt(maximum)
        messages = deepcopy(self._chat_history) + [
            {"role": "user", "content": summary_prompt}
        ]
        summary = self._chat_completion(messages)

        # Save summary
        with open(self._summary_path, "w", encoding="utf-8") as file:
            file.write(summary)
